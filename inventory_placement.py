"""
Inventory Placement Optimization Module

Greedy heuristic to assign high-demand items to most accessible warehouse locations.

INPUT FILES (expected in current working directory):
  1. item_attributes.csv
      Required columns:
        - item_id (string/int)
        - demand_frequency (numeric, higher = more demand)
        - dimensions (string, e.g. "10x20x30" or "10*20*30" or "10 X 20 X 30") -> height x width x depth (units consistent with shelf size)
        - current_stock (int)
        Optional columns:
        - weight_per_unit (numeric, defaults to 1.0 if missing)
  2. warehouse_layout.csv
      Required columns:
        - location_id (string)
        - x_coord (numeric)
        - y_coord (numeric)
        - max_size (numeric)  (capacity in volume units)
        - max_weight (numeric) (capacity in weight units)
        - (optional) location_type (string; rows containing 'packing' (case-insensitive) will be excluded)
        - (optional) is_packing_station (bool/int; if True/1 excludes)

PACKING STATION:
  - Assumed at coordinate (0, 0) unless explicitly marked via a location_type containing 'packing'.
  - Distances for ranking shelves calculated via Euclidean distance to (0,0).

OUTPUT:
    - placement_recommendations.csv with columns (multi-item aware):
                item_id, recommended_location, allocated_volume, allocated_weight, remaining_size, remaining_weight
        Backward compatibility: if old file exists with only first two columns it will be extended going forward.

USAGE:
  Basic:
      python inventory_placement.py
  Optional arguments:
      --items path/to/item_attributes.csv
      --layout path/to/warehouse_layout.csv
      --output path/to/placement_recommendations.csv
      --verbose (enable detailed logging)

ASSUMPTIONS & NOTES:
  - Greedy algorithm: items sorted by demand_frequency descending; locations sorted by distance ascending.
  - Each location holds AT MOST one item (as requested by description). Extend logic if multi-item storage is needed.
  - Volume = height * width * depth parsed from dimensions string; parsing is forgiving (extracts the first 3 numbers).
  - Weight per unit defaults to 1.0 if missing.
  - current_stock * weight_per_unit must fit within max_weight.
  - Items with malformed dimensions or missing capacity data will be marked UNPLACED with a warning.

EXTENSIONS (future ideas):
  - Permit partial fills or multiple items per shelf with residual capacity tracking.
  - Add tie-breakers (e.g., ABC classification clusters, adjacency constraints).
  - Integrate travel time / aisle structure rather than pure Euclidean distance.

Author: Auto-generated by GitHub Copilot
"""
from __future__ import annotations

import argparse
import logging
import math
import os
import re
import sys
from dataclasses import dataclass
from typing import List, Optional

import pandas as pd

LOGGER = logging.getLogger("inventory_placement")

DIMENSION_REGEX = re.compile(r"(\d+(?:\.\d+)?)")  # captures numbers (ints or floats)


@dataclass
class ItemRecord:
    item_id: str
    demand_frequency: float
    height: float
    width: float
    depth: float
    volume: float
    current_stock: int
    weight_per_unit: float

    @property
    def total_weight(self) -> float:
        return self.current_stock * self.weight_per_unit


@dataclass
class LocationRecord:
    location_id: str
    x: float
    y: float
    max_size: float
    max_weight: float
    distance: float
    occupied: bool = False


def parse_dimensions(dimensions: str) -> Optional[List[float]]:
    if not isinstance(dimensions, str):
        return None
    nums = DIMENSION_REGEX.findall(dimensions.replace(",", " "))
    if len(nums) < 3:
        return None
    try:
        h, w, d = map(float, nums[:3])
        return [h, w, d]
    except ValueError:
        return None


def load_items(path: str) -> List[ItemRecord]:
    df = pd.read_csv(path)
    required = {"item_id", "demand_frequency", "dimensions", "current_stock"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Missing required item columns: {missing}")

    if "weight_per_unit" not in df.columns:
        df["weight_per_unit"] = 1.0

    items: List[ItemRecord] = []
    for idx, row in df.iterrows():
        dims = parse_dimensions(row["dimensions"])
        if not dims:
            LOGGER.warning("Item %s has invalid dimensions '%s' -> will be UNPLACED later", row["item_id"], row["dimensions"])
            # store with zero volume to trigger fail
            dims = [0.0, 0.0, 0.0]
        h, w, d = dims
        volume = h * w * d
        try:
            items.append(
                ItemRecord(
                    item_id=str(row["item_id"]),
                    demand_frequency=float(row["demand_frequency"]),
                    height=h,
                    width=w,
                    depth=d,
                    volume=volume,
                    current_stock=int(row["current_stock"]),
                    weight_per_unit=float(row["weight_per_unit"]),
                )
            )
        except Exception as e:  # noqa: BLE001
            LOGGER.error("Skipping item at row %s due to error: %s", idx, e)
    # Sort high demand -> low
    items.sort(key=lambda x: (-x.demand_frequency, x.item_id))
    return items


def is_packing_row(row: pd.Series) -> bool:
    # Various heuristics to detect packing station rows
    lt = str(row.get("location_type", "")).lower()
    lid = str(row.get("location_id", "")).lower()
    is_flag = str(row.get("is_packing_station", "")).lower() in {"1", "true", "yes"}
    x = row.get("x_coord", None)
    y = row.get("y_coord", None)
    coord_is_origin = False
    try:
        coord_is_origin = float(x) == 0 and float(y) == 0
    except Exception:  # noqa: BLE001
        pass
    return ("packing" in lt) or ("packing" in lid) or is_flag or coord_is_origin


def load_locations(path: str) -> List[LocationRecord]:
    df = pd.read_csv(path)
    required = {"location_id", "x_coord", "y_coord", "max_size", "max_weight"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Missing required location columns: {missing}")

    # Filter out packing station rows
    filtered = df[~df.apply(is_packing_row, axis=1)].copy()

    locations: List[LocationRecord] = []
    for idx, row in filtered.iterrows():
        try:
            x = float(row["x_coord"])
            y = float(row["y_coord"])
            distance = math.sqrt(x * x + y * y)
            locations.append(
                LocationRecord(
                    location_id=str(row["location_id"]),
                    x=x,
                    y=y,
                    max_size=float(row["max_size"]),
                    max_weight=float(row["max_weight"]),
                    distance=distance,
                )
            )
        except Exception as e:  # noqa: BLE001
            LOGGER.error("Skipping location row %s due to error: %s", idx, e)
    # Sort accessible (closest) -> far
    locations.sort(key=lambda loc: (loc.distance, loc.location_id))
    return locations


def _initialize_capacity_df(existing_path: str) -> pd.DataFrame:
    """Load existing placement file (if any) returning DataFrame with required columns.
    Older versions may only have item_id,recommended_location.
    """
    if not os.path.exists(existing_path):
        return pd.DataFrame(columns=[
            "item_id", "recommended_location", "allocated_volume", "allocated_weight", "remaining_size", "remaining_weight"
        ])
    df = pd.read_csv(existing_path)
    # Add missing columns with NaNs; we'll recompute residuals from scratch.
    for col in ["allocated_volume", "allocated_weight", "remaining_size", "remaining_weight"]:
        if col not in df.columns:
            df[col] = None
    return df


def greedy_assign(items: List[ItemRecord], locations: List[LocationRecord], existing_path: str) -> pd.DataFrame:
    """Multi-item greedy assign with residual capacity.
    Each location can host multiple items until size or weight exceeded.
    Residual capacities tracked in output rows.
    """
    existing_df = _initialize_capacity_df(existing_path)
    # Build capacity map: location_id -> remaining size/weight
    cap_map = {loc.location_id: {"remaining_size": loc.max_size, "remaining_weight": loc.max_weight} for loc in locations}
    # Subtract already allocated items (recompute from existing rows where placement valid)
    if not existing_df.empty:
        # We don't trust stored residuals (could be absent); recompute from allocations
        merged = existing_df.merge(
            pd.DataFrame([{ 'location_id': l.location_id, 'max_size': l.max_size, 'max_weight': l.max_weight } for l in locations]),
            left_on="recommended_location", right_on="location_id", how="left"
        )
        for _, row in merged.iterrows():
            loc_id = row.get("recommended_location")
            if not isinstance(loc_id, str) or loc_id == "UNPLACED" or loc_id not in cap_map:
                continue
            allocated_volume = row.get("allocated_volume")
            allocated_weight = row.get("allocated_weight")
            # Fallback: if missing allocated values, try to infer from item_id via provided volume approximations not stored -> skip
            if pd.isna(allocated_volume) or pd.isna(allocated_weight):
                # Cannot reliably subtract; skip to avoid negatives
                continue
            cap_map[loc_id]["remaining_size"] -= float(allocated_volume)
            cap_map[loc_id]["remaining_weight"] -= float(allocated_weight)
            # Clamp at zero
            cap_map[loc_id]["remaining_size"] = max(0.0, cap_map[loc_id]["remaining_size"])
            cap_map[loc_id]["remaining_weight"] = max(0.0, cap_map[loc_id]["remaining_weight"])

    placement_rows = []
    for item in items:
        chosen = None
        if item.volume <= 0:
            placement_rows.append({
                "item_id": item.item_id,
                "recommended_location": "UNPLACED",
                "allocated_volume": 0.0,
                "allocated_weight": 0.0,
                "remaining_size": None,
                "remaining_weight": None,
            })
            continue
        for loc in locations:
            cap = cap_map[loc.location_id]
            if item.volume <= cap["remaining_size"] and item.total_weight <= cap["remaining_weight"]:
                # Assign here
                cap["remaining_size"] -= item.volume
                cap["remaining_weight"] -= item.total_weight
                chosen = loc.location_id
                placement_rows.append({
                    "item_id": item.item_id,
                    "recommended_location": chosen,
                    "allocated_volume": item.volume,
                    "allocated_weight": item.total_weight,
                    "remaining_size": cap["remaining_size"],
                    "remaining_weight": cap["remaining_weight"],
                })
                break
        if not chosen:
            placement_rows.append({
                "item_id": item.item_id,
                "recommended_location": "UNPLACED",
                "allocated_volume": 0.0,
                "allocated_weight": 0.0,
                "remaining_size": None,
                "remaining_weight": None,
            })
    return pd.DataFrame(placement_rows)


def run(items_path: str, layout_path: str, output_path: str) -> str:
    LOGGER.info("Loading items from %s", items_path)
    items = load_items(items_path)
    LOGGER.info("Loaded %d items", len(items))
    LOGGER.info("Loading locations from %s", layout_path)
    locations = load_locations(layout_path)
    LOGGER.info("Loaded %d candidate locations", len(locations))

    if not items:
        raise RuntimeError("No valid items loaded. Aborting.")
    if not locations:
        LOGGER.warning("No locations available. All items will be UNPLACED.")

    LOGGER.info("Running greedy assignment...")
    df_result = greedy_assign(items, locations, output_path)
    # If existing file exists, append new rows (multi-item incremental). Else write new.
    if os.path.exists(output_path):
        existing = _initialize_capacity_df(output_path)
        combined = pd.concat([existing, df_result], ignore_index=True)
        combined.to_csv(output_path, index=False)
    else:
        df_result.to_csv(output_path, index=False)
    LOGGER.info("Placement recommendations written to %s", output_path)
    return output_path


def configure_logging(verbose: bool):
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format="%(asctime)s | %(levelname)-8s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def parse_args(argv: List[str]) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Inventory Placement Optimization (Greedy)")
    parser.add_argument("--items", default="inventory_data.csv", help="Path to item attributes CSV")
    parser.add_argument("--layout", default="locations_data.csv", help="Path to warehouse layout CSV")
    parser.add_argument("--output", default="placement_recommendations.csv", help="Output CSV path")
    parser.add_argument("--verbose", action="store_true", help="Enable debug logging")
    return parser.parse_args(argv)


def main(argv: Optional[List[str]] = None) -> int:
    args = parse_args(argv or sys.argv[1:])
    configure_logging(args.verbose)

    for p in [args.items, args.layout]:
        if not os.path.exists(p):
            LOGGER.error("Required input file not found: %s", p)
            return 1
    try:
        run(args.items, args.layout, args.output)
    except Exception as e:  # noqa: BLE001
        LOGGER.exception("Failed to generate placement recommendations: %s", e)
        return 1
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
